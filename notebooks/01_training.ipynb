{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e56a7c5c",
      "metadata": {
        "id": "e56a7c5c"
      },
      "source": [
        "## Configuraci√≥n del entorno de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ffcaa17",
      "metadata": {},
      "source": [
        "### Optimizaciones CUDA y configuraci√≥n del dispositivo\n",
        "\n",
        "Se configuran las optimizaciones para CUDA:\n",
        "- **cudnn.benchmark**: Optimiza autom√°ticamente los algoritmos de convoluci√≥n para el hardware espec√≠fico\n",
        "- **allow_tf32**: Permite usar TensorFloat-32 para acelerar operaciones en GPUs Ampere y superiores\n",
        "- **DEVICE**: Detecta autom√°ticamente si CUDA est√° disponible para usar GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "Jz4rpEz83yUs",
      "metadata": {
        "id": "Jz4rpEz83yUs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02c92b1c",
      "metadata": {},
      "source": [
        "### Montaje de Google Drive\n",
        "\n",
        "Esta celda monta Google Drive para acceder a los datos almacenados en la nube. \n",
        "**Nota**: Esta celda solo funciona en Google Colab. Para uso local, aseg√∫rate de tener los datos en la ruta correcta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "cS2AO7WQyplf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS2AO7WQyplf",
        "outputId": "a61d8dfd-0a1e-44fa-b2d5-1747ff12a4a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82566bae",
      "metadata": {
        "id": "82566bae"
      },
      "source": [
        "## Carga de datos por arquitectura"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a9340a",
      "metadata": {},
      "source": [
        "### Funciones para carga y preprocesamiento de datos\n",
        "\n",
        "Esta secci√≥n incluye:\n",
        "\n",
        "1. **WEIGHTS_MAP**: Mapeo de arquitecturas a sus pesos preentrenados oficiales\n",
        "2. **get_transforms()**: Obtiene las transformaciones oficiales para cada arquitectura\n",
        "3. **load_data_by_arch()**: Carga los datasets optimizados con:\n",
        "   - Transformaciones espec√≠ficas por arquitectura\n",
        "   - DataLoaders optimizados con `pin_memory`, `persistent_workers` y `prefetch_factor`\n",
        "   - Configuraci√≥n para train/validation/test splits\n",
        "\n",
        "**Optimizaciones implementadas**:\n",
        "- `num_workers=8`: Carga de datos en paralelo\n",
        "- `pin_memory=True`: Transferencia m√°s r√°pida CPU‚ÜíGPU\n",
        "- `persistent_workers=True`: Reutiliza workers entre epochs\n",
        "- `prefetch_factor=4`: Prebufferiza lotes para reducir latencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "8e6cc87c",
      "metadata": {
        "id": "8e6cc87c"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "WEIGHTS_MAP = {\n",
        "    \"resnet50\": models.ResNet50_Weights.DEFAULT,\n",
        "    \"efficientnet_b3\": models.EfficientNet_B3_Weights.DEFAULT,\n",
        "    \"mobilenet_v3_large\": models.MobileNet_V3_Large_Weights.DEFAULT,\n",
        "}\n",
        "\n",
        "def get_transforms(arch: str):\n",
        "    \"\"\"\n",
        "    Devuelve los transforms oficiales seg√∫n la arquitectura.\n",
        "    \"\"\"\n",
        "    arch = arch.lower()\n",
        "    weights = WEIGHTS_MAP[arch]\n",
        "    tfm = weights.transforms(antialias=True)\n",
        "    return tfm\n",
        "\n",
        "# --- DataLoaders optimizados ---\n",
        "def load_data_by_arch(\n",
        "    arch: str,\n",
        "    batch_size: int,\n",
        "    num_workers: int = 8,\n",
        "):\n",
        "    base = f\"/content/drive/MyDrive/data/versionB\"\n",
        "\n",
        "    t_train = get_transforms(arch)\n",
        "    t_eval  = get_transforms(arch)\n",
        "\n",
        "    train_ds = datasets.ImageFolder(f\"{base}/train\", transform=t_train)\n",
        "    val_ds   = datasets.ImageFolder(f\"{base}/val\",   transform=t_eval)\n",
        "    test_ds  = datasets.ImageFolder(f\"{base}/test\",  transform=t_eval)\n",
        "\n",
        "    common = dict(num_workers=num_workers,\n",
        "                  pin_memory=True,\n",
        "                  persistent_workers=True)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=batch_size, shuffle=True,\n",
        "        prefetch_factor=4, drop_last=True, **common\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=batch_size, shuffle=False, **common\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds, batch_size=batch_size, shuffle=False, **common\n",
        "    )\n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50330f0b",
      "metadata": {},
      "source": [
        "## Funciones de entrenamiento y evaluaci√≥n\n",
        "\n",
        "Esta secci√≥n contiene todas las funciones principales para el entrenamiento:\n",
        "\n",
        "### üèóÔ∏è Construcci√≥n del modelo (`build_model`)\n",
        "- Soporte para ResNet50, EfficientNet-B3 y MobileNet-V3-Large\n",
        "- Utiliza pesos preentrenados de ImageNet\n",
        "- Reemplaza la cabeza clasificadora con dropout personalizado\n",
        "- Aplica optimizaciones de memoria (`channels_last`, `torch.compile`)\n",
        "\n",
        "### üìä Evaluaci√≥n (`evaluate`)\n",
        "Calcula m√©tricas comprehensivas:\n",
        "- **B√°sicas**: Accuracy, Balanced Accuracy, F1-macro, Precision, Recall\n",
        "- **Avanzadas**: ROC-AUC macro, PR-AUC macro, Cohen's Kappa, Matthews Correlation\n",
        "- **Por clase**: Reporte detallado con precision/recall/F1 por categor√≠a\n",
        "- **Matriz de confusi√≥n**: Para an√°lisis detallado de errores\n",
        "\n",
        "### üöÄ Entrenamiento (`train_once`)\n",
        "Implementa t√©cnicas avanzadas:\n",
        "- **Learning rates diferenciales**: Cabeza (1e-3) vs Backbone (1e-4)\n",
        "- **Mixed Precision Training**: AMP para acelerar entrenamiento\n",
        "- **Label Smoothing**: Reduce overfitting (Œ±=0.05)\n",
        "- **ReduceLROnPlateau**: Reduce LR cuando F1-macro no mejora\n",
        "- **Early Stopping**: Para en base a F1-macro con paciencia de 5 epochs\n",
        "\n",
        "### üíæ Pipeline completo (`run_single_training`)\n",
        "Orquesta todo el proceso y guarda autom√°ticamente:\n",
        "- Mejor checkpoint del modelo\n",
        "- Historial de entrenamiento completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb9fd609",
      "metadata": {
        "id": "fb9fd609"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torchvision import models\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score, confusion_matrix,\n",
        "    balanced_accuracy_score, roc_auc_score, average_precision_score,\n",
        "    cohen_kappa_score, matthews_corrcoef, classification_report\n",
        ")\n",
        "import numpy as np\n",
        "import copy\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def build_model(arch: str, num_classes: int, dropout: float = 0.2, pretrained: bool = True):\n",
        "    arch = arch.lower()\n",
        "    if arch == \"resnet50\":\n",
        "        weights = models.ResNet50_Weights.DEFAULT if pretrained else None\n",
        "        m = models.resnet50(weights=weights)\n",
        "        in_f = m.fc.in_features\n",
        "        m.fc = nn.Sequential(nn.Dropout(dropout), nn.Linear(in_f, num_classes))\n",
        "\n",
        "    elif arch == \"efficientnet_b3\":\n",
        "        weights = models.EfficientNet_B3_Weights.DEFAULT if pretrained else None\n",
        "        m = models.efficientnet_b3(weights=weights)\n",
        "        in_f = m.classifier[1].in_features\n",
        "        m.classifier[1] = nn.Sequential(nn.Dropout(dropout), nn.Linear(in_f, num_classes))\n",
        "\n",
        "    elif arch == \"mobilenet_v3_large\":\n",
        "        weights = models.MobileNet_V3_Large_Weights.DEFAULT if pretrained else None\n",
        "        m = models.mobilenet_v3_large(weights=weights)\n",
        "        in_f = m.classifier[3].in_features\n",
        "        m.classifier[3] = nn.Sequential(nn.Dropout(dropout), nn.Linear(in_f, num_classes))\n",
        "\n",
        "    for p in m.parameters():\n",
        "        p.requires_grad = True\n",
        "    m = m.to(DEVICE).to(memory_format=torch.channels_last)\n",
        "    m = torch.compile(m, mode=\"reduce-overhead\", fullgraph=False)\n",
        "    return m\n",
        "\n",
        "\n",
        "def _to_device_cl(x: torch.Tensor) -> torch.Tensor:\n",
        "    return x.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, loader, criterion: nn.Module):\n",
        "    model.eval()\n",
        "    losses, y_true, y_prob = [], [], []\n",
        "    class_names = getattr(loader.dataset, \"classes\", None)\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = _to_device_cl(x)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        losses.append(loss.item())\n",
        "        y_true.extend(y.detach().cpu().tolist())\n",
        "        y_prob.extend(torch.softmax(logits, dim=1).detach().cpu().tolist())\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_prob = np.array(y_prob)      # [N, C]\n",
        "    y_pred = y_prob.argmax(axis=1)\n",
        "    C = y_prob.shape[1]\n",
        "\n",
        "    acc     = accuracy_score(y_true, y_pred)\n",
        "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "    f1m     = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    prem    = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    recm    = recall_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "    try:\n",
        "        auc_macro = roc_auc_score(y_true, y_prob, multi_class=\"ovr\", average=\"macro\")\n",
        "    except ValueError:\n",
        "        auc_macro = float(\"nan\")\n",
        "    try:\n",
        "        y_true_bin = label_binarize(y_true, classes=np.arange(C))\n",
        "        ap_macro = average_precision_score(y_true_bin, y_prob, average=\"macro\")\n",
        "    except ValueError:\n",
        "        ap_macro = float(\"nan\")\n",
        "\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "    mcc   = matthews_corrcoef(y_true, y_pred)\n",
        "\n",
        "    report = classification_report(\n",
        "        y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"loss\": float(np.mean(losses)),\n",
        "        \"acc\": float(acc),\n",
        "        \"bal_acc\": float(bal_acc),\n",
        "        \"f1_macro\": float(f1m),\n",
        "        \"precision_macro\": float(prem),\n",
        "        \"recall_macro\": float(recm),\n",
        "        \"auc_macro\": float(auc_macro),\n",
        "        \"ap_macro\": float(ap_macro),\n",
        "        \"kappa\": float(kappa),\n",
        "        \"mcc\": float(mcc),\n",
        "        \"per_class\": report,\n",
        "        \"cm\": confusion_matrix(y_true, y_pred),\n",
        "    }\n",
        "\n",
        "def train_once(model: nn.Module, train_loader, val_loader,\n",
        "               epochs: int = 30, lr_head: float = 1e-3, lr_backbone: float = 1e-4,\n",
        "               weight_decay: float = 1e-4, es_patience: int = 5, amp: bool = True):\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
        "\n",
        "    head_keys = (\"fc\", \"classifier\")\n",
        "    head_params, backbone_params = [], []\n",
        "    for n, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        (head_params if any(k in n for k in head_keys) else backbone_params).append(p)\n",
        "\n",
        "    optimizer = AdamW([\n",
        "        {\"params\": backbone_params, \"lr\": lr_backbone},\n",
        "        {\"params\": head_params,     \"lr\": lr_head}\n",
        "    ], weight_decay=weight_decay)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"max\", factor=0.5, patience=3\n",
        "    )\n",
        "    scaler = GradScaler(enabled=(amp and torch.cuda.is_available()))\n",
        "\n",
        "    best_f1, best_state, wait = -1.0, None, 0\n",
        "    history = []\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running, n = 0.0, 0\n",
        "        for x, y in train_loader:\n",
        "            x = x.to(DEVICE, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            y = y.to(DEVICE, non_blocking=True)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast(device_type=DEVICE.type, enabled=(amp and torch.cuda.is_available())):\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            bs = y.size(0)\n",
        "            running += loss.item() * bs\n",
        "            n += bs\n",
        "        train_loss = running / max(1, n)\n",
        "\n",
        "        valm = evaluate(model, val_loader, criterion)\n",
        "        scheduler.step(valm[\"f1_macro\"])\n",
        "        history.append({\"epoch\": ep, \"train_loss\": train_loss, **valm})\n",
        "        print(\n",
        "          f\"[{ep:02d}/{epochs}] \"\n",
        "          f\"train={train_loss:.4f} | val_loss={valm['loss']:.4f} \"\n",
        "          f\"acc={valm['acc']:.4f} balAcc={valm['bal_acc']:.4f} \"\n",
        "          f\"f1M={valm['f1_macro']:.4f} AUC={valm['auc_macro']:.4f} \"\n",
        "          f\"AP={valm['ap_macro']:.4f} \"\n",
        "          f\"kappa={valm['kappa']:.4f} mcc={valm['mcc']:.4f} \"\n",
        "      )\n",
        "\n",
        "        if valm[\"f1_macro\"] > best_f1:\n",
        "            best_f1 = valm[\"f1_macro\"]\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= es_patience:\n",
        "                print(f\"Early stopping en epoch {ep:02d} (mejor F1-macro={best_f1:.4f})\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model, history\n",
        "\n",
        "def run_single_training(arch: str, train_loader, val_loader, out_prefix: str):\n",
        "    num_classes = len(train_loader.dataset.classes)\n",
        "    class_to_idx = train_loader.dataset.class_to_idx\n",
        "    model = build_model(arch, num_classes=num_classes, dropout=0.2, pretrained=True)\n",
        "\n",
        "    model, hist = train_once(\n",
        "        model, train_loader, val_loader,\n",
        "        epochs=30, lr_head=1e-3, lr_backbone=1e-4, weight_decay=1e-4,\n",
        "        es_patience=5, amp=True\n",
        "    )\n",
        "\n",
        "    ckpt_path = f\"./{out_prefix}_{arch}_best.pth\"\n",
        "    torch.save({\n",
        "        \"arch\": arch,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"class_to_idx\": class_to_idx,\n",
        "    }, ckpt_path)\n",
        "\n",
        "    hist_path = f\"./{out_prefix}_{arch}_history.pt\"\n",
        "    torch.save({\"history\": hist}, hist_path)\n",
        "\n",
        "    print(f\"Guardado: {ckpt_path} | {hist_path}\")\n",
        "    return ckpt_path, hist_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6383de31",
      "metadata": {},
      "source": [
        "## üß™ Experimentos de entrenamiento\n",
        "\n",
        "En esta secci√≥n se ejecuta el entrenamiento para cada arquitectura de manera secuencial. Cada experimento:\n",
        "\n",
        "1. **Carga los datos** con transformaciones espec√≠ficas de la arquitectura\n",
        "2. **Entrena el modelo** con las mejores pr√°cticas implementadas\n",
        "3. **Guarda autom√°ticamente**:\n",
        "   - `run_vB_{arch}_best.pth`: Mejor checkpoint del modelo\n",
        "   - `run_vB_{arch}_history.pt`: Historial completo de m√©tricas\n",
        "\n",
        "**Configuraci√≥n com√∫n**:\n",
        "- Batch size: 32\n",
        "- Epochs m√°ximos: 30 (con early stopping)\n",
        "- Optimizer: AdamW con weight decay\n",
        "- Scheduler: ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "520b5f0d",
      "metadata": {},
      "source": [
        "### ResNet50\n",
        "**ResNet50** es una arquitectura cl√°sica con conexiones residuales que permite entrenar redes muy profundas. \n",
        "- Utiliza bloques residuales para evitar el problema del gradiente que desaparece\n",
        "- Excelente baseline para tareas de clasificaci√≥n de im√°genes\n",
        "- Balance √≥ptimo entre precisi√≥n y eficiencia computacional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "VSPURDsi0qc4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSPURDsi0qc4",
        "outputId": "b4d5c238-2b35-433f-8b2f-3a9d978f5ca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[01/30] train=0.5700 | val_loss=0.4797 acc=0.8417 balAcc=0.8152 f1M=0.7889 AUC=0.9476 AP=0.8509 kappa=0.7422 mcc=0.7465 \n",
            "[02/30] train=0.2476 | val_loss=0.4944 acc=0.8745 balAcc=0.7561 f1M=0.7748 AUC=0.9360 AP=0.8347 kappa=0.7844 mcc=0.7924 \n",
            "[03/30] train=0.2121 | val_loss=0.4566 acc=0.8745 balAcc=0.7968 f1M=0.8057 AUC=0.9502 AP=0.8594 kappa=0.7885 mcc=0.7890 \n",
            "[04/30] train=0.1940 | val_loss=0.4407 acc=0.8759 balAcc=0.8166 f1M=0.8205 AUC=0.9531 AP=0.8612 kappa=0.7922 mcc=0.7923 \n",
            "[05/30] train=0.1862 | val_loss=0.4558 acc=0.8799 balAcc=0.8044 f1M=0.8160 AUC=0.9461 AP=0.8467 kappa=0.7975 mcc=0.7984 \n",
            "[06/30] train=0.1855 | val_loss=0.4510 acc=0.8759 balAcc=0.7901 f1M=0.8065 AUC=0.9475 AP=0.8534 kappa=0.7892 mcc=0.7913 \n",
            "[07/30] train=0.1822 | val_loss=0.4445 acc=0.8786 balAcc=0.8060 f1M=0.8180 AUC=0.9505 AP=0.8609 kappa=0.7951 mcc=0.7960 \n",
            "[08/30] train=0.1814 | val_loss=0.4351 acc=0.8786 balAcc=0.8160 f1M=0.8212 AUC=0.9542 AP=0.8645 kappa=0.7964 mcc=0.7966 \n",
            "[09/30] train=0.1800 | val_loss=0.4400 acc=0.8827 balAcc=0.8025 f1M=0.8159 AUC=0.9488 AP=0.8559 kappa=0.8016 mcc=0.8030 \n",
            "[10/30] train=0.1797 | val_loss=0.4433 acc=0.8745 balAcc=0.7930 f1M=0.8056 AUC=0.9500 AP=0.8523 kappa=0.7877 mcc=0.7888 \n",
            "[11/30] train=0.1780 | val_loss=0.4501 acc=0.8786 balAcc=0.8029 f1M=0.8148 AUC=0.9509 AP=0.8565 kappa=0.7949 mcc=0.7957 \n",
            "[12/30] train=0.1770 | val_loss=0.4500 acc=0.8813 balAcc=0.7950 f1M=0.8118 AUC=0.9440 AP=0.8473 kappa=0.7985 mcc=0.8008 \n",
            "[13/30] train=0.1767 | val_loss=0.4374 acc=0.8827 balAcc=0.8000 f1M=0.8168 AUC=0.9513 AP=0.8566 kappa=0.8008 mcc=0.8027 \n",
            "Early stopping en epoch 13 (mejor F1-macro=0.8212)\n",
            "Guardado: ./run_vB_resnet50_best.pth | ./run_vB_resnet50_history.pt\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./run_vB_resnet50_best.pth', './run_vB_resnet50_history.pt')"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_loader, val_loader, _ = load_data_by_arch(arch=\"resnet50\", batch_size=32)\n",
        "run_single_training(\"resnet50\", train_loader, val_loader, out_prefix=\"run_vB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a631a3f",
      "metadata": {},
      "source": [
        "### EfficientNet-B3\n",
        "\n",
        "**EfficientNet-B3** utiliza compound scaling para optimizar simult√°neamente profundidad, ancho y resoluci√≥n:\n",
        "- Arquitectura m√°s eficiente computacionalmente que ResNet\n",
        "- Mejor precisi√≥n con menos par√°metros mediante el uso de escalado compuesto\n",
        "- Incorpora squeeze-and-excitation blocks para mejorar la representaci√≥n de caracter√≠sticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "5ad778cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ad778cd",
        "outputId": "83abbd66-d4fb-4e4f-fa9b-eef8bd65ff55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-b3899882.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47.2M/47.2M [00:00<00:00, 211MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[01/30] train=0.5422 | val_loss=0.4287 acc=0.8759 balAcc=0.8415 f1M=0.8255 AUC=0.9568 AP=0.8756 kappa=0.7964 mcc=0.7976 \n",
            "[02/30] train=0.3331 | val_loss=0.4726 acc=0.9018 balAcc=0.8432 f1M=0.8487 AUC=0.9405 AP=0.8489 kappa=0.8357 mcc=0.8361 \n",
            "[03/30] train=0.2643 | val_loss=0.5213 acc=0.8759 balAcc=0.7646 f1M=0.7824 AUC=0.9189 AP=0.8295 kappa=0.7875 mcc=0.7924 \n",
            "[04/30] train=0.2255 | val_loss=0.4322 acc=0.8936 balAcc=0.8116 f1M=0.8236 AUC=0.9616 AP=0.8810 kappa=0.8205 mcc=0.8223 \n",
            "[05/30] train=0.2130 | val_loss=0.4526 acc=0.8895 balAcc=0.8383 f1M=0.8351 AUC=0.9501 AP=0.8496 kappa=0.8163 mcc=0.8163 \n",
            "[06/30] train=0.2120 | val_loss=0.4795 acc=0.8827 balAcc=0.7751 f1M=0.7924 AUC=0.9371 AP=0.8484 kappa=0.7996 mcc=0.8042 \n",
            "[07/30] train=0.1983 | val_loss=0.4478 acc=0.8936 balAcc=0.8110 f1M=0.8241 AUC=0.9541 AP=0.8662 kappa=0.8202 mcc=0.8218 \n",
            "Early stopping en epoch 07 (mejor F1-macro=0.8487)\n",
            "Guardado: ./run_vB_efficientnet_b3_best.pth | ./run_vB_efficientnet_b3_history.pt\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./run_vB_efficientnet_b3_best.pth', './run_vB_efficientnet_b3_history.pt')"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_loader, val_loader, _ = load_data_by_arch(arch=\"efficientnet_b3\", batch_size=32)\n",
        "run_single_training(\"efficientnet_b3\", train_loader, val_loader, out_prefix=\"run_vB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e48cbd24",
      "metadata": {},
      "source": [
        "### MobileNet-V3-Large\n",
        "\n",
        "**MobileNet-V3-Large** est√° optimizada para dispositivos m√≥viles y aplicaciones con recursos limitados:\n",
        "- Utiliza depthwise separable convolutions para reducir el costo computacional\n",
        "- Incorpora hard-swish activation y squeeze-and-excitation blocks\n",
        "- Ideal para deployment en producci√≥n donde la latencia es cr√≠tica\n",
        "- Mantiene buena precisi√≥n con un modelo significativamente m√°s peque√±o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "e-fWGhoz4V86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-fWGhoz4V86",
        "outputId": "278a82e0-888b-48fb-92fe-3e8eee184509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-5c1a4163.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-5c1a4163.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21.1M/21.1M [00:00<00:00, 198MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[01/30] train=0.5240 | val_loss=0.5100 acc=0.8513 balAcc=0.8111 f1M=0.8051 AUC=0.9431 AP=0.8540 kappa=0.7552 mcc=0.7577 \n",
            "[02/30] train=0.3412 | val_loss=0.4459 acc=0.8827 balAcc=0.8125 f1M=0.8249 AUC=0.9569 AP=0.8778 kappa=0.8029 mcc=0.8063 \n",
            "[03/30] train=0.2523 | val_loss=0.4767 acc=0.8840 balAcc=0.7865 f1M=0.8025 AUC=0.9296 AP=0.8506 kappa=0.8032 mcc=0.8075 \n",
            "[04/30] train=0.2171 | val_loss=0.4277 acc=0.8950 balAcc=0.8522 f1M=0.8453 AUC=0.9578 AP=0.8876 kappa=0.8260 mcc=0.8262 \n",
            "[05/30] train=0.1994 | val_loss=0.4424 acc=0.9004 balAcc=0.8521 f1M=0.8509 AUC=0.9491 AP=0.8580 kappa=0.8341 mcc=0.8341 \n",
            "[06/30] train=0.1921 | val_loss=0.4507 acc=0.9018 balAcc=0.8451 f1M=0.8480 AUC=0.9506 AP=0.8610 kappa=0.8360 mcc=0.8362 \n",
            "[07/30] train=0.1964 | val_loss=0.4655 acc=0.8827 balAcc=0.8475 f1M=0.8319 AUC=0.9531 AP=0.8559 kappa=0.8069 mcc=0.8084 \n",
            "[08/30] train=0.1906 | val_loss=0.4441 acc=0.8990 balAcc=0.8340 f1M=0.8419 AUC=0.9414 AP=0.8624 kappa=0.8306 mcc=0.8311 \n",
            "[09/30] train=0.1823 | val_loss=0.4296 acc=0.8977 balAcc=0.8349 f1M=0.8434 AUC=0.9499 AP=0.8619 kappa=0.8282 mcc=0.8286 \n",
            "[10/30] train=0.1795 | val_loss=0.4401 acc=0.8977 balAcc=0.8306 f1M=0.8382 AUC=0.9464 AP=0.8548 kappa=0.8283 mcc=0.8289 \n",
            "Early stopping en epoch 10 (mejor F1-macro=0.8509)\n",
            "Guardado: ./run_vB_mobilenet_v3_large_best.pth | ./run_vB_mobilenet_v3_large_history.pt\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./run_vB_mobilenet_v3_large_best.pth',\n",
              " './run_vB_mobilenet_v3_large_history.pt')"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_loader, val_loader, _ = load_data_by_arch(arch=\"mobilenet_v3_large\", batch_size=32)\n",
        "run_single_training(\"mobilenet_v3_large\", train_loader, val_loader, out_prefix=\"run_vB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1841bff",
      "metadata": {},
      "source": [
        "## üìã Resumen y pr√≥ximos pasos\n",
        "\n",
        "### ‚úÖ Resultados del entrenamiento\n",
        "\n",
        "Al completar estos experimentos, tendr√°s:\n",
        "\n",
        "1. **Tres modelos entrenados** con diferentes arquitecturas\n",
        "2. **Checkpoints guardados** con los mejores pesos de cada modelo\n",
        "3. **Historial completo** de m√©tricas para an√°lisis posterior\n",
        "\n",
        "\n",
        "### üöÄ Pr√≥ximos pasos recomendados\n",
        "\n",
        "1. **An√°lisis comparativo**: Comparar m√©tricas entre arquitecturas\n",
        "2. **Evaluaci√≥n en test set**: Usar el notebook `02_eval_inf.ipynb` \n",
        "4. **An√°lisis de errores**: Examinar matrices de confusi√≥n\n",
        "5. **Inferencia**: Probar los modelos en nuevas im√°genes\n",
        "\n",
        "### üí° Notas importantes\n",
        "\n",
        "- Los modelos est√°n listos para inferencia y evaluaci√≥n\n",
        "- Se utiliz√≥ early stopping basado en F1-macro para evitar overfitting\n",
        "- Las transformaciones son espec√≠ficas de cada arquitectura para m√°ximo rendimiento"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "16ae1717"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
